{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification With BERT and KerasNLP\n\nNow since I am done building the sentiment analysis model using different algorithms, I will make use of BERT, a popular Masked Language Model which is bidirectional (it has access to the words left and right) to build a the text classification model and also KerasNLP, which provides a simple Keras API for training and finetuning NLP models to classify the sentiments.","metadata":{"tags":[]}},{"cell_type":"code","source":"# import the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport zipfile\nimport os\nimport string\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_nlp\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:54:47.231978Z","iopub.execute_input":"2023-09-27T08:54:47.232367Z","iopub.status.idle":"2023-09-27T08:55:04.821399Z","shell.execute_reply.started":"2023-09-27T08:54:47.232337Z","shell.execute_reply":"2023-09-27T08:55:04.819913Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"}]},{"cell_type":"code","source":"# load the exported data\ndf1 = pd.read_csv('/kaggle/input/sentiments/exported_sentiments.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:04.823111Z","iopub.execute_input":"2023-09-27T08:55:04.823719Z","iopub.status.idle":"2023-09-27T08:55:04.844854Z","shell.execute_reply.started":"2023-09-27T08:55:04.823690Z","shell.execute_reply":"2023-09-27T08:55:04.843707Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# encode the target labels\ndf1['Sentiments'] = df1['Sentiments'].replace({\n    'negative': 0,\n    'positive': 1\n})\ndf1['Sentiments'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:04.846059Z","iopub.execute_input":"2023-09-27T08:55:04.846463Z","iopub.status.idle":"2023-09-27T08:55:04.870701Z","shell.execute_reply.started":"2023-09-27T08:55:04.846429Z","shell.execute_reply":"2023-09-27T08:55:04.869249Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Sentiments\n0    59\n1    41\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"X = df1['Feedback']\ny = df1['Sentiments']","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:04.874141Z","iopub.execute_input":"2023-09-27T08:55:04.874541Z","iopub.status.idle":"2023-09-27T08:55:04.883712Z","shell.execute_reply.started":"2023-09-27T08:55:04.874504Z","shell.execute_reply":"2023-09-27T08:55:04.882637Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(y)\nprint()\nX.to_frame()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:04.885927Z","iopub.execute_input":"2023-09-27T08:55:04.886359Z","iopub.status.idle":"2023-09-27T08:55:04.913472Z","shell.execute_reply.started":"2023-09-27T08:55:04.886327Z","shell.execute_reply":"2023-09-27T08:55:04.912159Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0     1\n1     1\n2     0\n3     0\n4     0\n     ..\n95    1\n96    0\n97    0\n98    1\n99    1\nName: Sentiments, Length: 100, dtype: int64\n\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                             Feedback\n0   The man is too fast in his teaching,he clearly...\n1      The class is dry but he really puts in efforts\n2   The course is shit and it's a threat to my bra...\n3                He no try at all, didnâ€™t teach well.\n4                    Ogbeni you sef know as e dae go \n..                                                ...\n95                                 easy and no wahala\n96  terrible way of teaching with the I-dont-care ...\n97                                 do not like coding\n98   this practical is hard on top 1 unit course haba\n99              not the right way to teach, Mr Akanni\n\n[100 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feedback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The man is too fast in his teaching,he clearly...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The class is dry but he really puts in efforts</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The course is shit and it's a threat to my bra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>He no try at all, didnâ€™t teach well.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ogbeni you sef know as e dae go</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>easy and no wahala</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>terrible way of teaching with the I-dont-care ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>do not like coding</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>this practical is hard on top 1 unit course haba</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>not the right way to teach, Mr Akanni</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:04.915260Z","iopub.execute_input":"2023-09-27T08:55:04.915571Z","iopub.status.idle":"2023-09-27T08:55:05.478511Z","shell.execute_reply.started":"2023-09-27T08:55:04.915546Z","shell.execute_reply":"2023-09-27T08:55:05.477397Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Text Preprocessing of the texts column using NLTK\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    text = re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n    text = ''.join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    return ' '.join(tokens)\n\nX_preprocessed = [preprocess_text(text) for text in X]\n\n# Split the preprocessed data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:05.480027Z","iopub.execute_input":"2023-09-27T08:55:05.480367Z","iopub.status.idle":"2023-09-27T08:55:06.919370Z","shell.execute_reply.started":"2023-09-27T08:55:05.480342Z","shell.execute_reply":"2023-09-27T08:55:06.918347Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(pd.Series(X_preprocessed), y, test_size=0.25)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:06.922030Z","iopub.execute_input":"2023-09-27T08:55:06.922699Z","iopub.status.idle":"2023-09-27T08:55:06.929501Z","shell.execute_reply.started":"2023-09-27T08:55:06.922667Z","shell.execute_reply":"2023-09-27T08:55:06.928136Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(75,) (75,)\n(25,) (25,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert labels to one-hot encoded format\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=2, dtype='float32')\ny_test = tf.keras.utils.to_categorical(y_test, num_classes=2, dtype='float32')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:06.930804Z","iopub.execute_input":"2023-09-27T08:55:06.931136Z","iopub.status.idle":"2023-09-27T08:55:06.943512Z","shell.execute_reply.started":"2023-09-27T08:55:06.931108Z","shell.execute_reply":"2023-09-27T08:55:06.942390Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# load the pretrained BERT model that has been finetuned for sentiment analysis\n\nmodel_name = \"bert_tiny_en_uncased_sst2\"\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    model_name,\n    num_classes=2,\n    load_weights = True,\n    activation='sigmoid' # for the binary classification task\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:06.947783Z","iopub.execute_input":"2023-09-27T08:55:06.948153Z","iopub.status.idle":"2023-09-27T08:55:11.145700Z","shell.execute_reply.started":"2023-09-27T08:55:06.948125Z","shell.execute_reply":"2023-09-27T08:55:11.144832Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased_sst2/v1/vocab.txt\n231508/231508 [==============================] - 0s 1us/step\nDownloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased_sst2/v1/model.h5\n17596448/17596448 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The next step is to compile and train the model. The aim here is to use the pre-trained model and finetune it on the dataset.","metadata":{}},{"cell_type":"code","source":"classifier.compile(\n    loss=keras.losses.BinaryCrossentropy(),\n    optimizer=keras.optimizers.Adam(),\n    jit_compile=True,\n     metrics=[\"accuracy\"],\n)\n# Access backbone programatically (e.g., to change `trainable`).\nclassifier.backbone.trainable = False\n# Fit again.\nclassifier.fit(x=X_train, y=y_train, validation_data=(X_test,y_test), batch_size=64)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-27T08:55:11.148856Z","iopub.execute_input":"2023-09-27T08:55:11.149726Z","iopub.status.idle":"2023-09-27T08:55:42.754692Z","shell.execute_reply.started":"2023-09-27T08:55:11.149695Z","shell.execute_reply":"2023-09-27T08:55:42.753664Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 26s 10s/step - loss: 0.4940 - accuracy: 0.7600 - val_loss: 0.3557 - val_accuracy: 0.8800\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7c6ecf0c60b0>"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate the model on the testing data\nclassifier.evaluate(X_test, y_test,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:55:42.756148Z","iopub.execute_input":"2023-09-27T08:55:42.757345Z","iopub.status.idle":"2023-09-27T08:55:44.987131Z","shell.execute_reply.started":"2023-09-27T08:55:42.757308Z","shell.execute_reply":"2023-09-27T08:55:44.986164Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step - loss: 0.3557 - accuracy: 0.8800\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[0.3557356894016266, 0.8799999952316284]"},"metadata":{}}]},{"cell_type":"code","source":"# checking the model to see performance on new samples\nsentiment_categories = [\"negative\", \"positive\"]\n\nnew_examples = list(df1['Feedback'].sample(10))\n\nscores = classifier.predict([preprocess_text(example) for example in new_examples])\n\nfor i, score in enumerate(scores):\n    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:00:12.399210Z","iopub.execute_input":"2023-09-27T09:00:12.399528Z","iopub.status.idle":"2023-09-27T09:00:14.355494Z","shell.execute_reply.started":"2023-09-27T09:00:12.399505Z","shell.execute_reply":"2023-09-27T09:00:14.354476Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 858ms/step\nlove to code and course is about coding. A plus for me:âž¡ negative with a 81.2 percent confidence.\n\nThe course is shit and it's a threat to my brain,the teaching mode is so poor :âž¡ negative with a 93.55 percent confidence.\n\ngreat teaching method from lecturer:âž¡ negative with a 64.99 percent confidence.\n\nnice:âž¡ positive with a 77.77 percent confidence.\n\nthis course is hard:âž¡ negative with a 92.44 percent confidence.\n\nHe no try at all, didnâ€™t teach well.:âž¡ negative with a 84.03 percent confidence.\n\nAkanni, you are a bad teacher wtf:âž¡ negative with a 93.4 percent confidence.\n\nI just hope I pass this course cos omo:âž¡ negative with a 91.12 percent confidence.\n\nI struggled at the start but it all went easy as time goes by:âž¡ negative with a 81.65 percent confidence.\n\nThe teaching mode is okay as the lecturer do revision of what's being taught from time to time.:âž¡ negative with a 80.12 percent confidence.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Improving model accuracy","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\n# Define a learning rate scheduler\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n\n# During model fitting\nclassifier.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=32, callbacks=[lr_scheduler])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:00:18.823008Z","iopub.execute_input":"2023-09-27T09:00:18.823533Z","iopub.status.idle":"2023-09-27T09:00:41.250575Z","shell.execute_reply.started":"2023-09-27T09:00:18.823498Z","shell.execute_reply":"2023-09-27T09:00:41.249383Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 12s 2s/step - loss: 0.4955 - accuracy: 0.7467 - val_loss: 0.3510 - val_accuracy: 0.9200 - lr: 0.0010\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7c6ea8a22950>"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate the model on the testing data\nclassifier.evaluate(X_test, y_test,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:00:41.252027Z","iopub.execute_input":"2023-09-27T09:00:41.252300Z","iopub.status.idle":"2023-09-27T09:00:43.489976Z","shell.execute_reply.started":"2023-09-27T09:00:41.252280Z","shell.execute_reply":"2023-09-27T09:00:43.488210Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step - loss: 0.3510 - accuracy: 0.9200\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[0.3510168194770813, 0.9200000166893005]"},"metadata":{}}]},{"cell_type":"code","source":"# checking the model to see performance on new samples\nsentiment_categories = [\"negative\", \"positive\"]\n\nnew_examples = list(df1['Feedback'].sample(10))\n\nscores = classifier.predict([preprocess_text(example) for example in new_examples])\n\nfor i, score in enumerate(scores):\n    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:00:58.222287Z","iopub.execute_input":"2023-09-27T09:00:58.222683Z","iopub.status.idle":"2023-09-27T09:01:00.175874Z","shell.execute_reply.started":"2023-09-27T09:00:58.222652Z","shell.execute_reply":"2023-09-27T09:01:00.174006Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 857ms/step\nOmooooo is all I can sayðŸ˜­:âž¡ negative with a 78.77 percent confidence.\n\ngood one:âž¡ positive with a 87.95 percent confidence.\n\nTeaching mode is bad but course is sometimes easy to understand:âž¡ negative with a 77.9 percent confidence.\n\nI learnt a lot in the course, but the lecturers are too demanding:âž¡ negative with a 68.98 percent confidence.\n\nhard and lecturer is fast when teaching:âž¡ negative with a 53.13 percent confidence.\n\nThe lecturer is good and his course is also good:âž¡ positive with a 82.27 percent confidence.\n\ndo not like coding:âž¡ negative with a 78.99 percent confidence.\n\nThe course is very very difficult and the lecturer no dey even make am easy.\nHe actually taught and explained 27 pages of the material within 3 hours.:âž¡ negative with a 75.1 percent confidence.\n\nlove to code:âž¡ positive with a 89.35 percent confidence.\n\nThe outline of the course is difficult and lecturer is bad:âž¡ negative with a 85.93 percent confidence.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Set some layers of the BERT backbone to trainable\n# classifier.backbone.layers[-3:].trainable = True\n\n# # Compile and fit the model again\n# classifier.compile(\n#     loss=keras.losses.BinaryCrossentropy(),\n#     optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # Adjust the learning rate\n#     metrics=[\"accuracy\"]\n# )\n# classifier.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=32, callbacks=[lr_scheduler])","metadata":{},"execution_count":163,"outputs":[]},{"cell_type":"markdown","source":"## Finetune BERT With User-controlled Preprocessing","metadata":{}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n    model_name,\n    sequence_length=128,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:02:17.903135Z","iopub.execute_input":"2023-09-27T09:02:17.903468Z","iopub.status.idle":"2023-09-27T09:02:18.311162Z","shell.execute_reply.started":"2023-09-27T09:02:17.903441Z","shell.execute_reply":"2023-09-27T09:02:18.310140Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"training_data = tf.data.Dataset.from_tensor_slices(([X_train], [y_train]))\nvalidation_data = tf.data.Dataset.from_tensor_slices(([X_test], [y_test]))\n\ntrain_cached = (\n    training_data.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)\ntest_cached = (\n    validation_data.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:02:20.422299Z","iopub.execute_input":"2023-09-27T09:02:20.423014Z","iopub.status.idle":"2023-09-27T09:02:22.571437Z","shell.execute_reply.started":"2023-09-27T09:02:20.422974Z","shell.execute_reply":"2023-09-27T09:02:22.569707Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Pretrained classifier.\nclassifier2 = keras_nlp.models.BertClassifier.from_preset(\n    model_name,\n    preprocessor=None,\n    num_classes=2,\n    load_weights = True,\n    activation='sigmoid'\n)\nclassifier2.compile(\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    optimizer=keras.optimizers.Adam(),\n    jit_compile=True,\n     metrics=[\"accuracy\"],\n)\nclassifier2.fit(train_cached, validation_data=test_cached,epochs=10)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-27T09:03:22.562385Z","iopub.execute_input":"2023-09-27T09:03:22.562732Z","iopub.status.idle":"2023-09-27T09:03:44.108189Z","shell.execute_reply.started":"2023-09-27T09:03:22.562707Z","shell.execute_reply":"2023-09-27T09:03:44.106922Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1/1 [==============================] - 13s 13s/step - loss: 0.4932 - accuracy: 0.7867 - val_loss: 0.2439 - val_accuracy: 0.9200\nEpoch 2/10\n1/1 [==============================] - 1s 829ms/step - loss: 0.4144 - accuracy: 0.7867 - val_loss: 0.2652 - val_accuracy: 0.9200\nEpoch 3/10\n1/1 [==============================] - 1s 735ms/step - loss: 0.2634 - accuracy: 0.9467 - val_loss: 0.3650 - val_accuracy: 0.8400\nEpoch 4/10\n1/1 [==============================] - 1s 729ms/step - loss: 0.2165 - accuracy: 0.9467 - val_loss: 0.4359 - val_accuracy: 0.8000\nEpoch 5/10\n1/1 [==============================] - 1s 741ms/step - loss: 0.1604 - accuracy: 0.9733 - val_loss: 0.4072 - val_accuracy: 0.8400\nEpoch 6/10\n1/1 [==============================] - 1s 738ms/step - loss: 0.1292 - accuracy: 0.9733 - val_loss: 0.3172 - val_accuracy: 0.8400\nEpoch 7/10\n1/1 [==============================] - 1s 736ms/step - loss: 0.0731 - accuracy: 0.9867 - val_loss: 0.2191 - val_accuracy: 0.9600\nEpoch 8/10\n1/1 [==============================] - 1s 733ms/step - loss: 0.0861 - accuracy: 0.9733 - val_loss: 0.2011 - val_accuracy: 0.9200\nEpoch 9/10\n1/1 [==============================] - 1s 732ms/step - loss: 0.0687 - accuracy: 0.9867 - val_loss: 0.2211 - val_accuracy: 0.9200\nEpoch 10/10\n1/1 [==============================] - 1s 752ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.8800\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7c6e795c20e0>"},"metadata":{}}]},{"cell_type":"code","source":"# checking the model to see performance on new samples\nsentiment_categories = [\"negative\", \"positive\"]\n\nnew_examples = list(df1['Feedback'].sample(30))\n\ntest_data =  preprocessor([preprocess_text(example) for example in new_examples])\nscores = classifier2.predict(test_data)\n\nfor i, score in enumerate(scores):\n    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:03:05.877464Z","iopub.execute_input":"2023-09-27T09:03:05.877803Z","iopub.status.idle":"2023-09-27T09:03:07.186223Z","shell.execute_reply.started":"2023-09-27T09:03:05.877780Z","shell.execute_reply":"2023-09-27T09:03:07.185163Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step\nneutral:âž¡ negative with a 98.74 percent confidence.\n\ngood:âž¡ positive with a 98.73 percent confidence.\n\neasy and no wahala:âž¡ positive with a 98.73 percent confidence.\n\nway to go. Nice job from lecturer:âž¡ positive with a 98.73 percent confidence.\n\nAwful & terrible from both the course and lecturer:âž¡ negative with a 98.74 percent confidence.\n\nthis course is hard:âž¡ negative with a 98.74 percent confidence.\n\nthank God for my coding skills  bruh:âž¡ negative with a 98.74 percent confidence.\n\nThe lecturer is fucking terrible. With his I-dont-care attitude towards students. The worst lecturer so far.:âž¡ negative with a 98.73 percent confidence.\n\nI hate this course plus the man:âž¡ negative with a 98.74 percent confidence.\n\nthe teaching method is so terrible :âž¡ negative with a 98.74 percent confidence.\n\npositive experience:âž¡ positive with a 98.72 percent confidence.\n\nApplied my math's knowledge from 200L for the most part of course:âž¡ negative with a 98.74 percent confidence.\n\nI felt the course was a bit rushed in terms of teaching the course and also the materials were a bit complex to read and understand :âž¡ negative with a 98.69 percent confidence.\n\nThe course is cool:âž¡ positive with a 98.73 percent confidence.\n\nhard and lecturer is fast when teaching:âž¡ negative with a 98.73 percent confidence.\n\nThe lecturer is good, I like him.:âž¡ positive with a 98.73 percent confidence.\n\nNa me dey make the class funny pass even though the man go do him best make am dry.:âž¡ positive with a 98.72 percent confidence.\n\nThat man no good at all:âž¡ positive with a 98.73 percent confidence.\n\nrelatively easy:âž¡ positive with a 98.73 percent confidence.\n\nThe course isnâ€™t supposed to be this difficult if the lecturer had taken his time to explain it and made the class an interactive one. :âž¡ negative with a 98.73 percent confidence.\n\nterrible way of teaching with the I-dont-care attitude:âž¡ negative with a 98.74 percent confidence.\n\nOgbeni you sef know as e dae go :âž¡ negative with a 98.74 percent confidence.\n\npracticals were okay for me sha:âž¡ positive with a 98.72 percent confidence.\n\ngood one:âž¡ positive with a 98.73 percent confidence.\n\nThe man is too fast in his teaching,he clearly doesn't know how to teach students very well.:âž¡ positive with a 98.72 percent confidence.\n\nSuch a terrible teaching method from the lecturer.:âž¡ negative with a 98.74 percent confidence.\n\nThe course is shit and it's a threat to my brain,the teaching mode is so poor :âž¡ negative with a 98.73 percent confidence.\n\nThis one-unit course has got a hell of wahala:âž¡ negative with a 98.61 percent confidence.\n\nThe course is very very difficult and the lecturer no dey even make am easy.\nHe actually taught and explained 27 pages of the material within 3 hours.:âž¡ negative with a 98.72 percent confidence.\n\nnice:âž¡ positive with a 98.73 percent confidence.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Saving models","metadata":{}},{"cell_type":"code","source":"# first model\nclassifier.save(\"sentiment_model1\", save_format='tf')\n\n# second model\nclassifier2.save(\"sentiment_model2\", save_format='tf')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:06:28.863963Z","iopub.execute_input":"2023-09-27T09:06:28.864387Z","iopub.status.idle":"2023-09-27T09:06:44.344054Z","shell.execute_reply.started":"2023-09-27T09:06:28.864356Z","shell.execute_reply":"2023-09-27T09:06:44.342368Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# first model\nclassifier.save(\"keras1\", save_format='keras')\n\n# second model\nclassifier2.save(\"keras2\", save_format='keras')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:20:11.650425Z","iopub.execute_input":"2023-09-27T09:20:11.650817Z","iopub.status.idle":"2023-09-27T09:20:27.492168Z","shell.execute_reply.started":"2023-09-27T09:20:11.650788Z","shell.execute_reply":"2023-09-27T09:20:27.490712Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Download saved models","metadata":{}},{"cell_type":"code","source":"directory_to_zip = \"/kaggle/working/keras1\"\noutput_zip_file = \"/kaggle/working/keras1.zip\"\n\n# Create a Zip file\nwith zipfile.ZipFile(output_zip_file, 'w') as zipf:\n    for root, dirs, files in os.walk(directory_to_zip):\n        for file in files:\n            zipf.write(os.path.join(root, file))\n            \nprint(f\"Zip file created: {output_zip_file}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:43:44.875700Z","iopub.execute_input":"2023-09-27T09:43:44.876154Z","iopub.status.idle":"2023-09-27T09:43:45.032303Z","shell.execute_reply.started":"2023-09-27T09:43:44.876115Z","shell.execute_reply":"2023-09-27T09:43:45.030713Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Zip file created: /kaggle/working/keras1.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\nimport os\n\ndirectory_to_zip = \"/kaggle/working/keras2\"\noutput_zip_file = \"/kaggle/working/keras2.zip\"\n\n# Create a Zip file\nwith zipfile.ZipFile(output_zip_file, 'w') as zipf:\n    for root, dirs, files in os.walk(directory_to_zip):\n        for file in files:\n            zipf.write(os.path.join(root, file))\n            \nprint(f\"Zip file created: {output_zip_file}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:48:13.602915Z","iopub.execute_input":"2023-09-27T09:48:13.603270Z","iopub.status.idle":"2023-09-27T09:48:13.744858Z","shell.execute_reply.started":"2023-09-27T09:48:13.603246Z","shell.execute_reply":"2023-09-27T09:48:13.743014Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Zip file created: /kaggle/working/keras2.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}