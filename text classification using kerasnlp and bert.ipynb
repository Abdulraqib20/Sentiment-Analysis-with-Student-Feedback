{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dff3f02-f5bd-41f7-a378-8677c1cc36ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Classification With BERT and KerasNLP\n",
    "\n",
    "Now since I am done building the sentiment analysis model using different algorithms, I will make use of BERT, a popular Masked Language Model which is bidirectional (it has access to the words left and right) to build a the text classification model and also KerasNLP, which provides a simple Keras API for training and finetuning NLP models to classify the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f5b9e43-deee-4c60-b2f4-bce602f295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b997453d-6adb-4bd4-ab19-de4648492a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the exported data\n",
    "df1 = pd.read_csv('exported_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1df6583e-3129-4906-b557-675b391edc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    59\n",
       "1    41\n",
       "Name: Sentiments, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the target labels\n",
    "df1['Sentiments'] = df1['Sentiments'].replace({\n",
    "    'negative': 0,\n",
    "    'positive': 1\n",
    "})\n",
    "df1['Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc7e9472-54b6-4f74-882b-34503485fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['Feedback']\n",
    "y = df1['Sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad8fe6ee-76a2-42bf-a9ba-3b464bef8e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     1\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    1\n",
      "96    0\n",
      "97    0\n",
      "98    1\n",
      "99    1\n",
      "Name: Sentiments, Length: 100, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The man is too fast in his teaching,he clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The class is dry but he really puts in efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The course is shit and it's a threat to my bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He no try at all, didnâ€™t teach well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ogbeni you sef know as e dae go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>easy and no wahala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>terrible way of teaching with the I-dont-care ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>do not like coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>this practical is hard on top 1 unit course haba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>not the right way to teach, Mr Akanni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Feedback\n",
       "0   The man is too fast in his teaching,he clearly...\n",
       "1      The class is dry but he really puts in efforts\n",
       "2   The course is shit and it's a threat to my bra...\n",
       "3                He no try at all, didnâ€™t teach well.\n",
       "4                    Ogbeni you sef know as e dae go \n",
       "..                                                ...\n",
       "95                                 easy and no wahala\n",
       "96  terrible way of teaching with the I-dont-care ...\n",
       "97                                 do not like coding\n",
       "98   this practical is hard on top 1 unit course haba\n",
       "99              not the right way to teach, Mr Akanni\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "print()\n",
    "X.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef566129-b075-460a-bed5-c0de23e3fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing of the texts column using NLTK\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_preprocessed = [preprocess_text(text) for text in X]\n",
    "\n",
    "# Split the preprocessed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4702f744-5d91-43cb-ba66-34a472e99e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75,) (75,)\n",
      "(25,) (25,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pd.Series(X_preprocessed), y, test_size=0.25)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "790757d4-7071-438d-8223-5f7418d3710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoded format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2, dtype='float32')\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ad28ec7-f6ca-4f54-b82b-6d98904388fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained BERT model that has been finetuned for sentiment analysis\n",
    "\n",
    "model_name = \"bert_tiny_en_uncased_sst2\"\n",
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    model_name,\n",
    "    num_classes=2,\n",
    "    load_weights = True,\n",
    "    activation='sigmoid' # for the binary classification task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8d1fb-525a-4408-8a26-7ea62ac8758c",
   "metadata": {},
   "source": [
    "The next step is to compile and train the model. The aim here is to use the pre-trained model and finetune it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69424199-83b5-4c6d-99dc-c03a5e814360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 54s 15s/step - loss: 0.5175 - accuracy: 0.7600 - val_loss: 0.2971 - val_accuracy: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1908006ec50>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    jit_compile=True,\n",
    "     metrics=[\"accuracy\"],\n",
    ")\n",
    "# Access backbone programatically (e.g., to change `trainable`).\n",
    "classifier.backbone.trainable = False\n",
    "# Fit again.\n",
    "classifier.fit(x=X_train, y=y_train, validation_data=(X_test,y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "890ac55d-4e8e-4200-9319-8ca926b15339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.2971 - accuracy: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29710203409194946, 0.8399999737739563]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on the testing data\n",
    "classifier.evaluate(X_test, y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2fff0c09-4ab3-4484-82a7-3b7d6ac527b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Lecturer is good but the class is dry and equations are much:âž¡ positive with a 74.52 percent confidence.\n",
      "\n",
      "love to code and course is about coding. A plus for me:âž¡ positive with a 61.06 percent confidence.\n",
      "\n",
      "The lecturer is good and his course is also good:âž¡ positive with a 83.17 percent confidence.\n",
      "\n",
      "Me wey be senior dev con dey struggle for LA class ha:âž¡ negative with a 90.4 percent confidence.\n",
      "\n",
      "We are basically left to go study that course on our own. :âž¡ negative with a 88.04 percent confidence.\n",
      "\n",
      "Applied my math's knowledge from 200L for the most part of course:âž¡ negative with a 67.25 percent confidence.\n",
      "\n",
      "stupid :âž¡ negative with a 91.4 percent confidence.\n",
      "\n",
      "stress no dey the course:âž¡ negative with a 88.66 percent confidence.\n",
      "\n",
      "very awful course:âž¡ negative with a 91.96 percent confidence.\n",
      "\n",
      "Omo, God will judge AK sha.\n",
      "You donâ€™t take a class like that and expect the students to understand what you are doing, thatâ€™s him and God ahead.\n",
      "Make I sha no fail:âž¡ negative with a 82.44 percent confidence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the model to see performance on new samples\n",
    "sentiment_categories = [\"negative\", \"positive\"]\n",
    "\n",
    "new_examples = list(df1['Feedback'].sample(10))\n",
    "\n",
    "scores = classifier.predict([preprocess_text(example) for example in new_examples])\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a84661-65a1-4096-bd3b-40e7ab923335",
   "metadata": {},
   "source": [
    "### Improving model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7b401cae-1180-42aa-9c16-83d870f6af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 18s 7s/step - loss: 0.3030 - accuracy: 0.8933 - val_loss: 0.2270 - val_accuracy: 0.9600 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190afd71f30>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# During model fitting\n",
    "classifier.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=32, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "185e0218-c56f-4e88-b2a5-2a85aaeb6bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.2270 - accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22701358795166016, 0.9599999785423279]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on the testing data\n",
    "classifier.evaluate(X_test, y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e3abb87-b9c3-4e24-8628-a32e31620cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Basically from my own experience, CPE 321 isnâ€™t hard but the method employed by the lecturer in teaching the course leaves much to be desired. I would have loved if the lecturer took his time and explained the course very well. :âž¡ positive with a 78.34 percent confidence.\n",
      "\n",
      "love to code:âž¡ positive with a 82.69 percent confidence.\n",
      "\n",
      "The lecturer is good and his course is also good:âž¡ positive with a 93.0 percent confidence.\n",
      "\n",
      "The course is shit and it's a threat to my brain,the teaching mode is so poor :âž¡ negative with a 92.5 percent confidence.\n",
      "\n",
      "relatively easy:âž¡ positive with a 83.17 percent confidence.\n",
      "\n",
      "terrible way of teaching with the I-dont-care attitude:âž¡ negative with a 92.58 percent confidence.\n",
      "\n",
      "very awful course:âž¡ negative with a 92.63 percent confidence.\n",
      "\n",
      "course should  not be difficult but lecturer messed it up:âž¡ negative with a 90.59 percent confidence.\n",
      "\n",
      "The lecturer is good, I like him.:âž¡ positive with a 92.95 percent confidence.\n",
      "\n",
      "the best course and lecturer:âž¡ positive with a 90.89 percent confidence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the model to see performance on new samples\n",
    "sentiment_categories = [\"negative\", \"positive\"]\n",
    "\n",
    "new_examples = list(df1['Feedback'].sample(10))\n",
    "\n",
    "scores = classifier.predict([preprocess_text(example) for example in new_examples])\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "32fdbdca-c734-46d2-b6d2-b067ccc51f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set some layers of the BERT backbone to trainable\n",
    "# classifier.backbone.layers[-3:].trainable = True\n",
    "\n",
    "# # Compile and fit the model again\n",
    "# classifier.compile(\n",
    "#     loss=keras.losses.BinaryCrossentropy(),\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # Adjust the learning rate\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "# classifier.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=32, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128aa99-67f5-4997-8605-bd920b64181c",
   "metadata": {},
   "source": [
    "## Finetune BERT With User-controlled Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f1a9127e-b550-4111-976b-1d327b955ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    model_name,\n",
    "    sequence_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0585b51a-ef44-411f-bd5b-389c519ce983",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.data.Dataset.from_tensor_slices(([X_train], [y_train]))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices(([X_test], [y_test]))\n",
    "\n",
    "train_cached = (\n",
    "    training_data.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_cached = (\n",
    "    validation_data.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c3041c8-9a71-4b58-9d76-c93e80d076ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 29s 29s/step - loss: 0.5401 - accuracy: 0.7600 - val_loss: 0.3550 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4694 - accuracy: 0.8000 - val_loss: 0.3077 - val_accuracy: 0.8800\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3728 - accuracy: 0.8400 - val_loss: 0.2386 - val_accuracy: 0.9200\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2558 - accuracy: 0.9333 - val_loss: 0.2576 - val_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1996 - accuracy: 0.9467 - val_loss: 0.2943 - val_accuracy: 0.8800\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1725 - accuracy: 0.9733 - val_loss: 0.2555 - val_accuracy: 0.9200\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1049 - accuracy: 0.9867 - val_loss: 0.1916 - val_accuracy: 0.9600\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0811 - accuracy: 1.0000 - val_loss: 0.1681 - val_accuracy: 0.9600\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9600\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 0.1664 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190b7d07340>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrained classifier.\n",
    "classifier2 = keras_nlp.models.BertClassifier.from_preset(\n",
    "    model_name,\n",
    "    preprocessor=None,\n",
    "    num_classes=2,\n",
    "    load_weights = True,\n",
    "    activation='sigmoid'\n",
    ")\n",
    "classifier2.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    jit_compile=True,\n",
    "     metrics=[\"accuracy\"],\n",
    ")\n",
    "classifier2.fit(train_cached, validation_data=test_cached,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "de938aa8-89bf-464f-a461-4bee6c9062c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n",
      "one-unit course wey get problem. nonsense:âž¡ negative with a 96.11 percent confidence.\n",
      "\n",
      "The outline of the course is difficult and lecturer is bad:âž¡ negative with a 96.07 percent confidence.\n",
      "\n",
      "This lecturer really tried for us to be honest but with the teaching of hundreds of students at a time, the class is very rowdy and students are mostly left to study on their own.:âž¡ positive with a 95.93 percent confidence.\n",
      "\n",
      "Nice teaching from scientist:âž¡ positive with a 95.98 percent confidence.\n",
      "\n",
      "relatively easy:âž¡ positive with a 95.98 percent confidence.\n",
      "\n",
      "nice:âž¡ positive with a 95.96 percent confidence.\n",
      "\n",
      "Lecturer is good but the class is dry and equations are much:âž¡ positive with a 95.99 percent confidence.\n",
      "\n",
      "SImple course:âž¡ negative with a 95.99 percent confidence.\n",
      "\n",
      "This one-unit course has got a hell of wahala:âž¡ negative with a 96.08 percent confidence.\n",
      "\n",
      "The teaching mode and environment is top tier but the course itself requires deep thinking and reasoning especially questions seen in exam so the little learned during classes is certainly not enough to solve the problems, especially if you donâ€™t have prior experience with programming.:âž¡ negative with a 96.03 percent confidence.\n",
      "\n",
      "GSE was very okay:âž¡ positive with a 95.83 percent confidence.\n",
      "\n",
      "A bonus course for me:âž¡ negative with a 95.65 percent confidence.\n",
      "\n",
      "I felt the course was a bit rushed in terms of teaching the course and also the materials were a bit complex to read and understand :âž¡ negative with a 95.6 percent confidence.\n",
      "\n",
      "The course is not well taught. Worst still, we are writing codes on paper for exams.:âž¡ negative with a 96.1 percent confidence.\n",
      "\n",
      "nice teaching method from the lecturer:âž¡ positive with a 95.98 percent confidence.\n",
      "\n",
      "good one:âž¡ positive with a 95.97 percent confidence.\n",
      "\n",
      "Omo, God will judge AK sha.\n",
      "You donâ€™t take a class like that and expect the students to understand what you are doing, thatâ€™s him and God ahead.\n",
      "Make I sha no fail:âž¡ negative with a 96.08 percent confidence.\n",
      "\n",
      "neutral:âž¡ negative with a 96.09 percent confidence.\n",
      "\n",
      "great teaching method from lecturer:âž¡ positive with a 95.76 percent confidence.\n",
      "\n",
      "class is always rowdy when taking this course:âž¡ negative with a 96.05 percent confidence.\n",
      "\n",
      "The course is actually well detailed in the material the lecturer uses. The lecturer is also cool-headed and makes the class interactive albeit boring at times.:âž¡ negative with a 95.98 percent confidence.\n",
      "\n",
      "Omooooo is all I can sayðŸ˜­:âž¡ negative with a 95.71 percent confidence.\n",
      "\n",
      "good:âž¡ positive with a 95.98 percent confidence.\n",
      "\n",
      "Ogbeni you sef know as e dae go :âž¡ negative with a 95.83 percent confidence.\n",
      "\n",
      "very awful course:âž¡ negative with a 96.06 percent confidence.\n",
      "\n",
      "The lecturer is good, I like him.:âž¡ positive with a 96.01 percent confidence.\n",
      "\n",
      "Na me dey make the class funny pass even though the man go do him best make am dry.:âž¡ positive with a 95.99 percent confidence.\n",
      "\n",
      "awful practicals. course handlers are tough.:âž¡ negative with a 95.97 percent confidence.\n",
      "\n",
      "this practical is hard on top 1 unit course haba:âž¡ positive with a 94.94 percent confidence.\n",
      "\n",
      "The teaching mode is okay as the lecturer do revision of what's being taught from time to time.:âž¡ positive with a 95.63 percent confidence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the model to see performance on new samples\n",
    "sentiment_categories = [\"negative\", \"positive\"]\n",
    "\n",
    "new_examples = list(df1['Feedback'].sample(30))\n",
    "\n",
    "test_data =  preprocessor([preprocess_text(example) for example in new_examples])\n",
    "scores = classifier2.predict(test_data)\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"{new_examples[i]}:âž¡ {sentiment_categories[np.argmax(score)]} with a { (100 * np.max(score)).round(2) } percent confidence.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c7a2f989-ba5a-4d22-aad3-98c6249b7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define sentiment categories\n",
    "# sentiment_categories = [\"negative\", \"positive\"]\n",
    "\n",
    "# new_examples = list(df1['Feedback'].sample(30))\n",
    "\n",
    "# # Initialize a list to store predictions\n",
    "# predictions = []\n",
    "\n",
    "# # Preprocess and predict sentiment for each new example\n",
    "# for example in new_examples:\n",
    "#     # Preprocess the input data using your custom preprocessing function\n",
    "#     preprocessed_text = preprocess_text(example)\n",
    "    \n",
    "#     # Apply the BERT preprocessor to the preprocessed text\n",
    "#     test_data = preprocessor([preprocessed_text])\n",
    "    \n",
    "#     # Predict sentiment using the classifier\n",
    "#     scores = classifier2.predict(test_data)\n",
    "    \n",
    "#     # Determine sentiment category and confidence\n",
    "#     prediction = f\"{sentiment_categories[np.argmax(scores)]} with a {(100 * np.max(scores)).round(2)} percent confidence.\"\n",
    "    \n",
    "#     # Store the prediction\n",
    "#     predictions.append((example, prediction))\n",
    "\n",
    "# # Display examples and predictions\n",
    "# for example, prediction in predictions:\n",
    "#     print(f\"{example}: âž¡ {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360cc81-4485-4321-bbbd-a6add40a2df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
